\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref} % For hyperlinks
\usepackage{amsmath}  % For math symbols
\usepackage{booktabs} % For nicer tables
\usepackage{caption}  % Better caption formatting
\usepackage{float}    % For table positioning

\title{DS542 Midterm Report}
\author{Josh Yip (Kaggle Username: joshyippie)}
\date{March 2025}

\begin{document}

\maketitle

\section*{AI Disclosure Statement}

This project involved the use of publicly available pretrained models, large-scale datasets, and computational tools that leverage artificial intelligence (AI) and machine learning (ML). The following outlines how AI was utilized during the course of this project:

\begin{itemize}
    \item Training techniques such as label smoothing, augmentation strategies (e.g., RandAugment, Gaussian Blur, RandomErasing), learning rate scheduling (OneCycleLR), and early stopping were used to enhance model performance and stability. These were informed by GPT as possible increasers for accuracy.

        \item Debugging PyTorch-related errors, including issues with installation, module loading, environment activation, and runtime exceptions (e.g., torchvision version compatibility, PyTorch GPU utilization).
        \item Model architecture selection and recommendations (e.g., choosing ResNeXt50, modifying initial convolution layers for CIFAR-100 image sizes).
        \item Hyperparameter tuning strategy advice (learning rates, optimizer settings, OneCycleLR scheduling).
        \item Advanced data augmentation recommendations (RandAugment, Gaussian Blur, RandomErasing, ColorJitter).
        \item Best practices for experiment tracking using \texttt{Weights \& Biases (wandb)}.
        \item Documentation and report-writing suggestions (structuring LaTeX sections clearly, writing comprehensive disclosures and analysis sections).
    \end{itemize}

\subsection*{Explicit Breakdown of Code Authorship:}

\begin{itemize}
    \item \textbf{Written entirely by me:}
    \begin{itemize}
        \item Core training and validation loops (\texttt{train} and \texttt{validate} functions).
        \item Data loading, pre-processing, and dynamic normalization logic.
        \item Early stopping logic and checkpoint.      
        \item Initial transformation steps (random cropping, resize, normalize).
    \end{itemize}
    
    \item \textbf{Written with partial AI assistance:}
    \begin{itemize}
        \item Selection and adaptation of the pretrained model (\texttt{get\_model} function).
        \item Advanced augmentation pipeline setup (RandAugment, RandomErasing).
        \item Hyperparameter tuning guidance (learning rate, epochs, optimizer choice, learning rate scheduler settings).
        \item Troubleshooting PyTorch environment issues (import errors, CUDA settings, cluster environment management).
    \end{itemize}

    \item \textbf{Directly AI-suggested improvements:}
    \begin{itemize}
        \item LaTeX document structuring, including detailed AI disclosure, design justification, results analysis, and ablation sections.
        \item Suggested clarifications for training progress interpretation (accuracy expectations, early stopping recommendations).
    \end{itemize}
\end{itemize}

\subsection*{Code Commenting and Documentation:}
\begin{itemize}
    \item All sections of the provided Python code contain clear, detailed comments describing their explicit purposes, as can be seen directly within the code files.
    \item Comments written by me explicitly indicate what each function and line does clearly, as recommended by GPT-4 to enhance readability.
\end{itemize}
    \textbf{Ethical and Fair Use:} No confidential, proprietary, or sensitive data was used. All third-party models, datasets, and APIs were used in accordance with their respective licenses.


\vspace{1em}
AI support contributed to rapid iteration, improved model interpretability, and enhanced documentation quality.

\section*{1. Experimental Overview}

This report presents a series of experiments conducted on the CIFAR-100 image classification dataset. The goal was to progressively improve test accuracy using architectural enhancements, data augmentation, optimization techniques, and transfer learning.

\section*{2. Observations and Analysis of Results}

\subsection*{2.1 Baseline (Simple CNN + SGD)}
\begin{itemize}
    \item \textbf{Optimizer:} SGD (learning rate = 0.001)
    \item \textbf{Test Accuracy:} 27.96\%

\end{itemize}

\subsection*{2.2 Simple CNN + Adam}
\begin{itemize}
    \item \textbf{Optimizer:} Adam (learning rate = 1e-3)
    \item \textbf{Test Accuracy:} 40.44\%
    \item \textbf{Analysis:}  The initial simple CNN model established baseline performance. Switching from SGD to Adam resulted in substantial accuracy improvements (27.96\% to 40.44\%), highlighting the importance of adaptive optimization in early training stages.

\end{itemize}

\subsection*{2.3 ResNet18 (Pretrained, Frozen)}
\begin{itemize}
    \item \textbf{Test Accuracy:} 44.57\%
    \item Even without fine-tuning, pretrained models showed improved performance over scratch models.
\end{itemize}

\subsection*{2.4 ResNet18 + Augmentation}
\begin{itemize}
    \item \textbf{Epochs:} 5
    \item \textbf{Test Accuracy:} 55.32\%
    \item Stronger augmentation boosted generalization significantly.
\end{itemize}

\subsection*{2.5 DenseNet121 + Augmentation + Adam}
\begin{itemize}
    \item \textbf{Epochs:} 10
    \item \textbf{Test Accuracy:} 76.00\%
    \item DenseNet provided better representational power and convergence.
\end{itemize}

\subsection*{2.6 ResNeXt50\_32x4d + Label Smoothing + Advanced Augmentation}
\begin{itemize}
    \item \textbf{Epochs:} 50
    \item \textbf{Local Test Accuracy:} 82.71\%
    \item \textbf{Leaderboard Submission Accuracy:} 54.48\%
    \item Achieved the highest test accuracy locally; however, performance dropped on final submission, likely due to domain shift or overfitting to local validation.
\end{itemize}

\section*{3. Ablation Study}

\begin{table}[H]
\centering
\begin{tabular}{@{}|l|c|@{}}
\hline
\textbf{Model Variant} & \textbf{CIFAR-100 Test Accuracy} \\
\hline
Simple CNN + SGD & 27.96\% \\
Simple CNN + Adam & 40.44\% \\
ResNet18 (pretrained, frozen) & 44.57\% \\
ResNet18 + Aug + Dynamic Norm & 55.32\% \\
DenseNet121 + Adam + 10 Epochs & 76.00\% \\
ResNeXt50\_32x4d + Aug + Smoothing & \textbf{82.71\%} \\
\hline
\end{tabular}
\caption{Comparison of model variants and their corresponding performance on CIFAR-100.}
\end{table}

\section*{4. Design Justification}

This section outlines the rationale behind the final configuration used in the model pipeline for CIFAR-100 classification.

\subsection*{4.1 Model Architecture: ResNeXt50\_32x4d}
\begin{itemize}
    \item \textbf{Why ResNeXt?} ResNeXt50\_32x4d is a powerful extension of ResNet that employs grouped convolutions to increase model capacity without significant computational overhead.
    \item \textbf{Dynamic Normalization:} Normalizing using CIFAR-100 dataset-specific mean and standard deviation led to more stable training and better performance than using generic $(0.5, 0.5, 0.5)$.
    \item \textbf{Pretraining Benefits:} The model was initialized with \texttt{ImageNet1K\_V1} pretrained weights to leverage transfer learning, which significantly boosts generalization performance on small datasets like CIFAR-100.
    \item \textbf{Input Resolution Compatibility:} CIFAR-100 uses 32x32 images, so the first convolution layer (originally designed for larger inputs) was modified with kernel size 3 and stride 1. Additionally, the aggressive downsampling via max-pooling was removed to preserve spatial details. Considered upscaling, but believed accuracy impact vs training time wasn't ideal for upscaling over freezing pooling layer.
    \item \textbf{Classifier Adaptation:} The final fully connected layer was replaced with a new head projecting to 100 classes.
\end{itemize}

\subsection*{4.2 Data Augmentation Strategy}
\begin{itemize}
    \item \textbf{Purpose:} Improve model robustness and generalization to unseen variations by simulating distortions during training.
    \item \textbf{Techniques Used:}
    \begin{itemize}
        \item \textbf{Random Crop \& Horizontal Flip} — Standard spatial augmentations for image recognition tasks.
        \item \textbf{ColorJitter} — Simulates lighting variation.
        \item \textbf{Gaussian Blur \& RandomErasing} — Encourage spatial and texture invariance.
        \item \textbf{RandAugment} — Plug-and-play policy to introduce strong randomized augmentations.
    \end{itemize}
    \item \textbf{Validation \& Test Sets:} No augmentation was applied to these sets to ensure fair performance evaluation.
\end{itemize}

\subsection*{4.3 Training Configuration}
\begin{itemize}
    \item \textbf{Optimizer:} SGD with momentum ($0.8$) and weight decay to promote generalization and stability.
    \item \textbf{Learning Rate Scheduler:} OneCycleLR was selected to allow for an aggressive warm-up and decay, which improves convergence speed and test performance.
    \item \textbf{Loss Function:} CrossEntropyLoss with label smoothing ($0.05$) helps prevent overconfidence and improves calibration.
    \item \textbf{Early Stopping:} A patience of 50 epochs with early stoppage was chosen, saving computation and reducing overfitting in cases of unproductive epochs.
\end{itemize}

\subsection*{4.4 Experiment Management}
\begin{itemize}
    \item \textbf{Tracking:} \texttt{Weights \& Biases (wandb)} was used to log accuracy, loss, and learning rate in real-time, enabled rapid debugging and model comparison.
    \item \textbf{Reproducibility:} A fixed seed ($42$) was set, and all models were saved conditionally based on validation performance.
\end{itemize}


\section*{5. Conclusion}

Each successive experiment demonstrated incremental improvements through better architecture choices, transfer learning, augmentation, and optimization techniques. The final model, ResNeXt50\_32x4d, achieved the best performance locally, though further tuning and regularization are necessary to bridge the gap between local and public leaderboard results.

\end{document}
